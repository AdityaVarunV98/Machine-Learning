{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Project Overview:**\n",
        "\n",
        "input: Image\n",
        "\n",
        "output: Caption\n",
        "\n",
        "Train the model using given dataset\n",
        "\n",
        "Steps:\n",
        "1. Split into train, val, test.\n",
        "2. Tune hyperparameters based on the validation set. Try out various hyperparameters. Prevent overfitting using common techniques (dropout, regularization)\n",
        "\n",
        "Model Choices:\n",
        "1. CNN for image feature extraction, and use this as the input for the RNNs. Incorporate attention into the image feature extraction.\n",
        "\n",
        "Code Structure: (Import the required modules)\n",
        "1. Input the data\n",
        "2. Preprocess the data\n",
        "3. Define the model\n",
        "4. Train the model. Use various different hyperparameters and choose the best for the validation set\n",
        "5. Evaluate using the test data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "99BmiStpkgt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_5rLW70kVN5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from torchvision import models\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting the drive\n",
        "\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "id": "yXsnbP-hz0w8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Data, Data Preprocessing, Building Vocabulary\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "def build_vocab(captions_file, threshold=1):\n",
        "    counter = Counter()\n",
        "    df = pd.read_csv(captions_file, delimiter=',', header=None, names=['image', 'caption'])\n",
        "    for caption in df['caption']:\n",
        "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "        counter.update(tokens)\n",
        "\n",
        "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
        "\n",
        "    vocab = Vocabulary()\n",
        "    vocab.add_word('<pad>')\n",
        "    vocab.add_word('<start>')\n",
        "    vocab.add_word('<end>')\n",
        "    vocab.add_word('<unk>')\n",
        "\n",
        "    for word in words:\n",
        "        vocab.add_word(word)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "captions_file = '/content/drive/My Drive/image_captions.zip/captions.txt'\n",
        "vocab = build_vocab(captions_file)\n",
        "\n",
        "vgg16 = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, images_dir, captions_file, vocab, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.captions = pd.read_csv(captions_file, delimiter=',', header=None, names=['image', 'caption'])\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "        self.feature_extractor = nn.Sequential(*list(models.vgg16(pretrained=True).children())[:-1])\n",
        "        self.feature_extractor.eval()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.images_dir, self.captions.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        caption = self.captions.iloc[idx, 1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = self.feature_extractor(image.unsqueeze(0))\n",
        "        features = features.view(-1)\n",
        "\n",
        "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
        "        caption_indices = [self.vocab('<start>')] + [self.vocab(token) for token in tokens] + [self.vocab('<end>')]\n",
        "        caption_tensor = torch.Tensor(caption_indices).long()\n",
        "\n",
        "        return features, caption_tensor\n",
        "\n",
        "images_dir = '/content/drive/My Drive/image_captions.zip/Images'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images for input to VGG16\n",
        "])\n",
        "\n",
        "dataset = ImageCaptionDataset(images_dir=images_dir, captions_file=captions_file, vocab=vocab, transform=transform)\n",
        "\n",
        "def collate_fn(data):\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    lengths = [len(cap) for cap in captions]\n",
        "    max_length = max(lengths)\n",
        "    padded_captions = torch.zeros(len(captions), max_length).long()\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        padded_captions[i, :end] = cap[:end]\n",
        "\n",
        "    return images, padded_captions\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "def tensor_to_caption(tensor, vocab):\n",
        "    words = []\n",
        "    for idx in tensor[0]:\n",
        "        word = vocab.idx2word[idx.item()]\n",
        "        if word == '<end>':\n",
        "            break\n",
        "        if word != '<start>' and word != '<pad>':\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "for images, captions in dataloader:\n",
        "    print(images.size())\n",
        "    print(captions)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "e4vtMPZVwxoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting into train, val, test datasets\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "train_size = 400\n",
        "val_size = int(0.8 * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(val_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "id": "ISyf_P_O7hVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "\n",
        "# Model: Many - Many LSTM RNN\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "\n",
        "embed_size = 25088\n",
        "hidden_size = 256\n",
        "vocab_size = len(vocab)\n",
        "num_layers = 1\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 2\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim = 1)\n",
        "\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        outputs = self.softmax(outputs)\n",
        "\n",
        "        #_, outputs = torch.max(outputs, dim=2)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, features, vocabulary, max_length = 50):\n",
        "        result = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = features\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length):\n",
        "\n",
        "                hiddens, states = self.lstm(x, states)\n",
        "                output = self.linear(hiddens)\n",
        "                output = self.softmax(output.unsqueeze(1))\n",
        "                predicted = output.argmax(2)\n",
        "                result.append(predicted)\n",
        "\n",
        "                x = self.embed(predicted)\n",
        "                x = x.squeeze(0)\n",
        "                if vocabulary.idx2word[predicted.item()] == \"<end>\":\n",
        "                    break\n",
        "        result_new = [vocabulary.idx2word[idx.item()] for idx in result]\n",
        "        return result_new"
      ],
      "metadata": {
        "id": "PlR7YKGNLTzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Model\n",
        "\n",
        "model = RNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = vocab(\"<pad>\"))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "m96190w8aOR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (imgs, captions) in enumerate(train_loader):\n",
        "\n",
        "        print(i)\n",
        "\n",
        "        imgs = imgs.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        lengths = [len(cap) for cap in captions]\n",
        "        lengths = torch.tensor(lengths).to(device)\n",
        "\n",
        "        outputs = model(imgs, captions)\n",
        "\n",
        "        packed_targets = pack_padded_sequence(captions, lengths, batch_first=True, enforce_sorted=False).data\n",
        "        outputs = pack_padded_sequence(outputs, lengths, batch_first=True, enforce_sorted=False).data\n",
        "\n",
        "        targets = packed_targets.view(-1)\n",
        "\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        print(\"Batch Loss:\")\n",
        "        print(running_loss)\n",
        "        running_loss=0\n",
        "        '''\n",
        "        if i % 2000 == 1999:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "        '''"
      ],
      "metadata": {
        "id": "F2RVdAx_Ze2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the BLEU score\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Ensure NLTK punkt is downloaded\n",
        "\n",
        "nltk.download('punkt')\n",
        "def calculate_bleu_scores(model, data_loader, vocab):\n",
        "    model.eval()\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "    bleu_scores = []\n",
        "    c = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, captions in test_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            features = imgs\n",
        "            for i in range(imgs.size(0)):\n",
        "                print(c)\n",
        "                c+=1\n",
        "                reference_caption = captions[i].tolist()\n",
        "                reference_caption = [vocab.idx2word[idx] for idx in reference_caption if (idx != vocab.word2idx['<pad>'] and idx != vocab.word2idx['<start>'] and idx != vocab.word2idx['<end>'])]\n",
        "                reference_caption = [reference_caption]\n",
        "                print(reference_caption)\n",
        "\n",
        "\n",
        "                feature = features[i].unsqueeze(0)\n",
        "                generated_caption_idx = model.caption_image(feature, vocab)\n",
        "\n",
        "                generated_caption = generated_caption_idx\n",
        "                print(generated_caption)\n",
        "\n",
        "                # generated_caption = [vocab.idx2word[idx] for idx in generated_caption_idx] -- Converts into words, but above is already in words based on the function\n",
        "\n",
        "                bleu_score = sentence_bleu(reference_caption, generated_caption, smoothing_function=smoothing_function)\n",
        "                print(bleu_score)\n",
        "                bleu_scores.append(bleu_score)\n",
        "\n",
        "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "    return avg_bleu_score\n",
        "\n",
        "bleu_score = calculate_bleu_scores(model, test_loader, vocab)\n",
        "print(f'Average BLEU score: {bleu_score}')"
      ],
      "metadata": {
        "id": "R4wUMph6avHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SOuwUUekrOc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}