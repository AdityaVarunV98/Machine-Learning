{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVmT47LOBy6i"
      },
      "source": [
        "\n",
        " <center> <h1> <b> Pattern Recognition and Machine Learning (EE5610 - EE2802 - AI2000 - AI5000) </b> </h1> </center>\n",
        "\n",
        "<b> Programming Assignment 01 - KNN : </b> Welcome to the programming assignment (PA) on k-nearest neighbors (KNN) classification. Throughout this PA, you will explore the k-NN algorithm, a versatile and intuitive method for tackling classification and regression challenges. Specifically, this assignment aims to enhance your understanding of the KNN classification algorithm. In this PA, we expect you to implement and experiment with the KNN classifier to understand how variations in 'k' and distance metrics influence classification performance.\n",
        "\n",
        "<b> Instructions </b>\n",
        "1. Plagiarism is strictly prohibited.\n",
        "2. Delayed submissions are not accepted\n",
        "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
        "\n",
        "\n",
        "\n",
        "<b> Part(a): Synthetic data generation </b>  \n",
        "\n",
        "1. Consider four bivariate Gaussians with means at (0,0), (0,1), (1,0), and (1,1), each having a variance of 0.3. Sample 90 points from each Gaussian, resulting in a total of 360 points. Allocate 30 points from each Gaussian for training and 60 points for testing. This results in a total of 120 points for training and 240 points for testing.\n",
        "2. Create a 2-class training set ($[X_{train}, Y_{train}]$) and a test set ($[X_{test}, Y_{test}]$) by labeling the data sampled from Gaussians with means at (0,0) and (1,1) as class 1, and the data sampled from Gaussians with means at (0,1) and (1,0) as class 2. Assign a label of +1 to class1 and -1 to class2.\n",
        "\n",
        "4. Visualize both train and test sets using the scatter plot on a 2-D plane. Indicate the data points from class 1 with a green color and those from class 2 with a blue color.\n",
        "\n",
        "<b> Part(b): KNN Classification - </b> The k-Nearest Neighbors (KNN) classifier algorithm is a straightforward yet powerful tool for classification tasks. The KNN classifier takes the test data point, computes distances to all points in the training set, identifies the 'k' nearest neighbors based on these distances, and assigns the test data to the class that the majority of its neighbors belong to.\n",
        "\n",
        "<b> Programming questions </b>\n",
        "\n",
        "\n",
        "1. Write a function called kNNClassify that accepts training data, a test point, and the hyperparameter 'k' as input and returns the label of the test point. Pick a reasonable 'k' for this experiment. Use \"kNNClassify\" function to generate the labels for the test data generated in part(a) of this PA. Compare the predicted labels with the original labels and calculate the portion of test data points that are correctly classified. In other words, calculate the accuracy of the classifier.\n",
        "3. Create a visual representation of the predictions by plotting all data points in a 2D plane. Assign green and blue colors to represent class 1 and class 2, respectively. For test data points that are misclassified, assign the color red.\n",
        "4. Generate and visualize the decision regions of 2D plane that are associated with each class, for a given classifier. Decision regions can be created by classifying all the data points in the 2D grid and assigning class-specific colors to them.\n",
        "\n",
        "<b> Part(c): Parameter selection: What is good value for k? - </b> One intuitive approach to determine the optimal 'k' is through cross-validation. During cross-validation, a ρ% portion of the training dataset is utilized as the validation dataset, and the model's performance is assessed on this validation set with various 'k' values. Through these cross-validation experiments, we select the 'k' that yields the best performance on the validation data.\n",
        "\n",
        "<b> Algorithm </b>\n",
        "\n",
        "1. Perform hold-out cross-validation by setting aside a fraction (ρ of the training set for validation. Note: You may use ρ = 0.3, and repeat the procedure 10 times. The hold-out procedure may be quite unstable.\n",
        "2. Use a large range of candidate values for k (e.g. k = 1, 3, 5..., 21). Notice odd numbers are considered to avoid ties.\n",
        "3. Repeat the process for 10 times using a random cross-validation set each time with a ρ = 0.3.\n",
        "4. Plot the training and validation errors for the different values of k.\n",
        "\n",
        "<b> Questions </b>\n",
        "\n",
        "5. How would you now answer the question \"what is the best value for k\"?\n",
        "6. How is the value of k affected by ρ (percentage of points held out) and number of repetitions? What does a large number of repetitions provide?\n",
        "7. Apply the model obtained by cross-validation (i.e., best k) to the test set and check\n",
        "if there is an improvement on the classification error over the result of Part 2.\n",
        "\n",
        "<b> Part(d): Influence of training data on KNN classifier - </b>\n",
        "\n",
        "1. Evaluate the performance as the size of the training set\n",
        "grows, e.g., n = {200, 400, 1200,...}. How would you choose a good range for k as n changes? What can you say about the stability of the solution? Check by repeating the validation multiple times.\n",
        "2. Try classifying more difficult datasets, for instance, by increasing the variance or adding noise by randomly flipping the labels on the training set.\n",
        "\n",
        "<b> Part(e): What is the influence of distance measure on decision regions ? - </b>\n",
        "\n",
        "1. Evaluate the performance of the KNN classifier with different distance measures such as $l_{1}$, $l_{2}$, etc,.\n",
        "2. Plot the decision regions of the KNN classifier with different distance measures.\n",
        "3. Report your observations.  \n",
        "\n",
        "<b> Part(f): MNIST Digit classification using KNN classifier: </b> : This part will not be graded. However, you are recommended to work on it to get exposure to the practical applications of the KNN classifier.\n",
        "\n",
        "1. Modify the function kNNClassify to handle multi-class problems and hence design a KNN classifier to classify the images in MNIST dataset as one of the 10 digits. The 28x28 images may be flattened to arrive at a 784 dimensional vector. NOTE: If you had already written a kNNClassify for multi class classification in part1 (b), you are free to use it.\n",
        "2. The MNIST dataset consists of approximately 70,000 images of handwritten digits. Create training, validation, and test datasets from this entire dataset with the respective proportions of 80%, 10%, and 10%.\n",
        "3. Empirically determine the most suitable error function, and the corresponding k to maximize the performance on the cross-validation experiments.\n",
        "4. Apply these values to evaluate the performance on the test dataset.\n",
        "5. Create a confusion matrix to understand the most confused classes (digits).\n",
        "6. Suggest alternate ways to improve the performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCAeXbdvgFVN"
      },
      "outputs": [],
      "source": [
        "#All imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjgkoGQOEjis"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "#Part(a) #Synthetic data generation\n",
        "########################################\n",
        "#Define means and covariances\n",
        "mean1=np.array([0, 0])\n",
        "mean2=np.array([0, 1])\n",
        "mean3=np.array([1, 0])\n",
        "mean4=np.array([1, 1])\n",
        "cov=np.array([[0.3, 0], [0, 0.3]])\n",
        "\n",
        "#Sample data points from the bivariate Gaussian distribution\n",
        "#You can use \"np.random.multivariate_normal\" function to sample the data points from the multivariate Gaussian distribution\n",
        "X1 = np.random.multivariate_normal(mean1, cov, 90)\n",
        "X2 = np.random.multivariate_normal(mean2, cov, 90)\n",
        "X3 = np.random.multivariate_normal(mean3, cov, 90)\n",
        "X4 = np.random.multivariate_normal(mean4, cov, 90)\n",
        "\n",
        "#Generate training data\n",
        "X_train = np.concatenate((X1[:30], X2[:30], X3[:30], X4[:30]))\n",
        "Y_train = np.concatenate((np.repeat(np.array([1]), 30), np.repeat(np.array([-1]), 60), np.repeat(np.array([1]), 30)))\n",
        "training_set = [X_train, Y_train]\n",
        "\n",
        "#Generate testing data\n",
        "X_test = np.concatenate((X1[30:90], X2[30:90], X3[30:90], X4[30:90]))\n",
        "Y_test = np.concatenate((np.repeat(np.array([1]), 60), np.repeat(np.array([-1]), 120), np.repeat(np.array([1]), 60)))\n",
        "test_set = [X_test, Y_test]\n",
        "\n",
        "#Visualize the data using plt.scatter() function\n",
        "colours_train = []\n",
        "colours_test = []\n",
        "for i in range(len(Y_train)):\n",
        "    if(Y_train[i]==1):\n",
        "        colours_train.append(\"green\")\n",
        "    else:\n",
        "        colours_train.append(\"blue\")\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "    if(Y_test[i]==1):\n",
        "        colours_test.append(\"green\")\n",
        "    else:\n",
        "        colours_test.append(\"blue\")\n",
        "\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c = colours_train)\n",
        "plt.title(\"Training Data Points\")\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c = colours_test)\n",
        "plt.title(\"Testing Data Points\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH91uokhDV_5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "#Part(b) #KNN classification\n",
        "########################################\n",
        "\n",
        "#Write \"kNNClassify\" function\n",
        "def kNNClassify(X_train,Y_train,X_test,k):\n",
        "    '''\n",
        "    #Inputs : Training data (X_train,Y_train), Test points  (X_test), Hyperparameter k\n",
        "    #Outputs : Predicted class\n",
        "    '''\n",
        "\n",
        "    predictions = []\n",
        "    possible_categories = [-1, 1]\n",
        "    for x_test in X_test:\n",
        "        x_test = x_test.reshape(x_test.shape[0], 1)\n",
        "        distances = np.sqrt(np.sum(np.square(X_train-x_test.T), axis = 1))\n",
        "        class_distance_pair = tuple(zip(list(Y_train), list(distances)))\n",
        "        k_nearest_categories = sorted(class_distance_pair, key=lambda x: x[1])[:k]\n",
        "        categories_hash = dict(zip(possible_categories, [0]*len(possible_categories)))\n",
        "        for category in k_nearest_categories:\n",
        "            categories_hash[category[0]] += 1\n",
        "\n",
        "        max_freq = 0\n",
        "        for key in categories_hash:\n",
        "            if(categories_hash[key] > max_freq):\n",
        "                max_freq_category = key\n",
        "                max_freq = categories_hash[key]\n",
        "        predictions.append(max_freq_category)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "#Write \"KNNAccuracy\" function\n",
        "def KNNAccuracy(true,pred):\n",
        "    '''\n",
        "    #Inputs : Ground truth and predicted labels\n",
        "    #Outputs : Portion of data points that are correctly classified, i.e., accuracy\n",
        "    '''\n",
        "\n",
        "    correct_predictions = 0\n",
        "    for i in range(len(pred)):\n",
        "        if(true[i] == pred[i]):\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions/len(pred)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "#Create a visual representation of predictions\n",
        "k = 5\n",
        "\n",
        "predictions = kNNClassify(X_train, Y_train, X_test, k)\n",
        "print(\"The accuracy for\", k, \"is\", KNNAccuracy(Y_test, predictions))\n",
        "\n",
        "predict_colours = []\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "    if(Y_test[i] != predictions[i]):\n",
        "        predict_colours.append(\"red\")\n",
        "    elif(Y_test[i]==1):\n",
        "        predict_colours.append(\"green\")\n",
        "    else:\n",
        "        predict_colours.append(\"blue\")\n",
        "\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c = predict_colours)\n",
        "plt.title(\"Predicted Test Data Points\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Generate and visualize the decision regions and overlay the test points\n",
        "res = 0.1\n",
        "\n",
        "min_i, max_i = X_test[:, 0].min()-1, X_test[:, 0].max()+1\n",
        "min_j, max_j = X_test[:, 0].min()-1, X_test[:, 0].max()+1\n",
        "\n",
        "i_grid = np.arange(min_i, max_i, res)\n",
        "j_grid = np.arange(min_j, max_j, res)\n",
        "\n",
        "ii, jj = np.meshgrid(i_grid, j_grid)\n",
        "ri, rj = ii.flatten(), jj.flatten()\n",
        "ri, rj = ri.reshape((len(ri), 1)), rj.reshape((len(rj), 1))\n",
        "\n",
        "grid = np.hstack((ri,rj))\n",
        "\n",
        "grid_predictions = kNNClassify(X_train, Y_train, grid, k)\n",
        "\n",
        "kk = np.array(grid_predictions)\n",
        "kk = kk.reshape(ii.shape)\n",
        "plt.contourf(ii, jj, kk, cmap = \"bone_r\")\n",
        "\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c = predict_colours)\n",
        "plt.title(\"Decision Regions with Test Points\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWmRWE8pDgnT"
      },
      "outputs": [],
      "source": [
        "####################################\n",
        "#Part(c): Parameter selection: What is good value for k?\n",
        "####################################\n",
        "#Write holdoutCVkNN() Function\n",
        "def holdoutCVkNN(k_range,numrep,rho):\n",
        "    training_errors = {}\n",
        "    validation_errors = {}\n",
        "    for k in range(k_range):\n",
        "        if k % 2 == 1:\n",
        "            training_error_k = []\n",
        "            validation_error_k = []\n",
        "            for i in range(numrep):\n",
        "                random_sample = random.sample(range(len(X_train)), int(rho*len(X_train)))\n",
        "                random_sample_set = set()\n",
        "                for i in random_sample:\n",
        "                    random_sample_set.add(i)\n",
        "\n",
        "                X_training_set = []\n",
        "                Y_training_set = []\n",
        "                X_validation_set = []\n",
        "                Y_validation_set = []\n",
        "\n",
        "                for j in range(len(X_train)):\n",
        "                    if j in random_sample_set:\n",
        "                        X_validation_set.append(X_train[j])\n",
        "                        Y_validation_set.append(Y_train[j])\n",
        "                    else:\n",
        "                        X_training_set.append(X_train[j])\n",
        "                        Y_training_set.append(Y_train[j])\n",
        "\n",
        "                X_training_set = np.array(X_training_set)\n",
        "                Y_training_set = np.array(Y_training_set)\n",
        "                X_validation_set = np.array(X_validation_set)\n",
        "                Y_validation_set = np.array(Y_validation_set)\n",
        "\n",
        "                predictions_train = kNNClassify(X_training_set, Y_training_set, X_training_set, k)\n",
        "                predictions_valid = kNNClassify(X_training_set, Y_training_set, X_validation_set, k)\n",
        "\n",
        "                training_accuracy = KNNAccuracy(Y_training_set, predictions_train)\n",
        "                validation_accuracy = KNNAccuracy(Y_validation_set, predictions_valid)\n",
        "\n",
        "                training_error = 1 - training_accuracy\n",
        "                validation_error = 1 - validation_accuracy\n",
        "\n",
        "                training_error_k.append(training_error)\n",
        "                validation_error_k.append(validation_error)\n",
        "\n",
        "            training_errors[k] = sum(training_error_k)/len(training_error_k)\n",
        "            validation_errors[k] = sum(validation_error_k)/len(validation_error_k)\n",
        "\n",
        "    return training_errors, validation_errors\n",
        "\n",
        "\n",
        "\n",
        "#Plot training and validation errors for different values of k\n",
        "k_range = 32\n",
        "numrep = 50\n",
        "rho = 0.3\n",
        "\n",
        "training_errors, validation_errors = holdoutCVkNN(k_range, numrep, rho)\n",
        "\n",
        "plt.scatter(training_errors.keys(), training_errors.values(), c = \"green\", label = \"Training Error\")\n",
        "plt.scatter(validation_errors.keys(), validation_errors.values(), c = \"blue\", label = \"Validation Error\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Errors in ascending order: \", dict(sorted(validation_errors.items(), key=lambda item: item[1])))\n",
        "\n",
        "\n",
        "'''\n",
        "    What is the best value for k?\n",
        " A: We can select the best value of k based on the least error obtained on average after the numrep iterations.\n",
        "\n",
        "    On running the above code, on average, the value of validation error is minimum for k = 13, with the values of k for\n",
        "    which the validation error is minimum ranging from 7 to 23 after around 12 runs. Hence we can conclude that the best\n",
        "    value for k is 13 on average. This for rho = 0.3, numrep = 10. The value of k for the minimum error has very high\n",
        "    variance when tested on different set of datapoints (same distribution). Hence, for this set of rho, numrep, it\n",
        "    is difficult to conclude the most optimal value for k\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "    Effect of rho on k:\n",
        "    On increasing the value of rho, the optimal value of k decreases on average (numrep = 10)\n",
        "    Data:\n",
        "        Rho                    Optimal Value of k:\n",
        "        0.3                    11, 11, 9, 15, 21, 7, 9, 19, 7, 23\n",
        "        0.45                   9, 3, 7, 7, 7, 9, 7, 7, 13, 7\n",
        "        0.6                    7, 5, 3, 5, 7, 3, 5, 5, 5, 7\n",
        "        0.75                   3, 3, 3, 5, 3, 5, 5, 3, 3, 5\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "    Effect of number repetitions on k:\n",
        "    On observing the graph for different values, we see that the validation error flattens out for larger values of numrep\n",
        "    This is due to averaging out the errors for larger number of iterations, causing a decrease in variance.\n",
        "\n",
        "    On increasing the number of repetitions, we can get a more accurate optimal value for k, since we consider more number of\n",
        "    iterations and the error averages out to the expected value of the error.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "#Evaluate the performance on test set with the best hyper parameters ( i.e best k ).\n",
        "\n",
        "k = min(validation_errors, key=validation_errors.get)\n",
        "print(\"The optimal value of k from the cross validation is\", k)\n",
        "\n",
        "predictions = kNNClassify(X_train, Y_train, X_test, k)\n",
        "print(\"The accuracy for this value of k:\", KNNAccuracy(Y_test, predictions))\n",
        "\n",
        "# Here we can observe an increase in the accuracy from the previous case to the optimal value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG8vSBuoEEYk"
      },
      "outputs": [],
      "source": [
        "##################################\n",
        "#Part(d): Influence of training data on KNN classifier\n",
        "##################################\n",
        "#Performance evaluation as n increases\n",
        "\n",
        "def data_generator(num_train_samples, randomizer = 0):\n",
        "    # num_train_samples is to be a multiple of 4, for the following division to work\n",
        "    # randomizer gives the flipping probability. If randomizer=1, all the values obtained from the original distribution flips\n",
        "\n",
        "    n = num_train_samples\n",
        "    mean1=np.array([0, 0])\n",
        "    mean2=np.array([0, 1])\n",
        "    mean3=np.array([1, 0])\n",
        "    mean4=np.array([1, 1])\n",
        "    cov=np.array([[0.3, 0], [0, 0.3]])\n",
        "\n",
        "    #Sample data points from the bivariate Gaussian distribution\n",
        "    #You can use \"np.random.multivariate_normal\" function to sample the data points from the multivariate Gaussian distribution\n",
        "    X1 = np.random.multivariate_normal(mean1, cov, int(3*n/4))\n",
        "    X2 = np.random.multivariate_normal(mean2, cov, int(3*n/4))\n",
        "    X3 = np.random.multivariate_normal(mean3, cov, int(3*n/4))\n",
        "    X4 = np.random.multivariate_normal(mean4, cov, int(3*n/4))\n",
        "\n",
        "    #Generate training data\n",
        "    X_train = np.concatenate((X1[:int(n/4)], X2[:int(n/4)], X3[:int(n/4)], X4[:int(n/4)]))\n",
        "    Y_train = np.concatenate((np.repeat(np.array([1]), int(n/4)), np.repeat(np.array([-1]), int(2*n/4)), np.repeat(np.array([1]), int(n/4))))\n",
        "\n",
        "    Y_train = np.multiply(Y_train, 2*np.random.binomial(1, 1-randomizer, n) - 1)\n",
        "\n",
        "    #Generate testing data\n",
        "    X_test = np.concatenate((X1[int(n/4):int(3*n/4)], X2[int(n/4):int(3*n/4)], X3[int(n/4):int(3*n/4)], X4[int(n/4):int(3*n/4)]))\n",
        "    Y_test = np.concatenate((np.repeat(np.array([1]), int(2*n/4)), np.repeat(np.array([-1]), n), np.repeat(np.array([1]), int(2*n/4))))\n",
        "\n",
        "    Y_test = np.multiply(Y_test, 2*np.random.binomial(1, 1-randomizer, 2*n) - 1)\n",
        "\n",
        "    data = {}\n",
        "    data[\"X_train\"] = X_train\n",
        "    data[\"Y_train\"] = Y_train\n",
        "    data[\"X_test\"] = X_test\n",
        "    data[\"Y_test\"] = Y_test\n",
        "\n",
        "    return data\n",
        "\n",
        "def holdoutCVkNN_large(k_range,numrep,rho, X_train, Y_train):\n",
        "    training_errors = {}\n",
        "    validation_errors = {}\n",
        "    for k in range(k_range):\n",
        "        if k % 2 == 1:\n",
        "            training_error_k = []\n",
        "            validation_error_k = []\n",
        "            for i in range(numrep):\n",
        "                random_sample = random.sample(range(len(X_train)), int(rho*len(X_train)))\n",
        "                random_sample_set = set()\n",
        "                for i in random_sample:\n",
        "                    random_sample_set.add(i)\n",
        "\n",
        "                X_training_set = []\n",
        "                Y_training_set = []\n",
        "                X_validation_set = []\n",
        "                Y_validation_set = []\n",
        "\n",
        "                for j in range(len(X_train)):\n",
        "                    if j in random_sample_set:\n",
        "                        X_validation_set.append(X_train[j])\n",
        "                        Y_validation_set.append(Y_train[j])\n",
        "                    else:\n",
        "                        X_training_set.append(X_train[j])\n",
        "                        Y_training_set.append(Y_train[j])\n",
        "\n",
        "                X_training_set = np.array(X_training_set)\n",
        "                Y_training_set = np.array(Y_training_set)\n",
        "                X_validation_set = np.array(X_validation_set)\n",
        "                Y_validation_set = np.array(Y_validation_set)\n",
        "\n",
        "                predictions_train = kNNClassify(X_training_set, Y_training_set, X_training_set, k)\n",
        "                predictions_valid = kNNClassify(X_training_set, Y_training_set, X_validation_set, k)\n",
        "\n",
        "                training_accuracy = KNNAccuracy(Y_training_set, predictions_train)\n",
        "                validation_accuracy = KNNAccuracy(Y_validation_set, predictions_valid)\n",
        "\n",
        "                training_error = 1 - training_accuracy\n",
        "                validation_error = 1 - validation_accuracy\n",
        "\n",
        "                training_error_k.append(training_error)\n",
        "                validation_error_k.append(validation_error)\n",
        "\n",
        "            training_errors[k] = sum(training_error_k)/len(training_error_k)\n",
        "            validation_errors[k] = sum(validation_error_k)/len(validation_error_k)\n",
        "\n",
        "    return training_errors, validation_errors\n",
        "\n",
        "\n",
        "n_values = [200, 400, 800, 1200]\n",
        "\n",
        "k_range = 45\n",
        "numrep = 10\n",
        "rho = 0.2\n",
        "optimal_k = []\n",
        "errors = []\n",
        "\n",
        "for n in n_values:\n",
        "    print(\"Current n=\", n)\n",
        "\n",
        "    data = data_generator(n)\n",
        "\n",
        "    X_train = data[\"X_train\"]\n",
        "    Y_train = data[\"Y_train\"]\n",
        "\n",
        "    train_error, validation_error = holdoutCVkNN_large(k_range, numrep, rho, X_train, Y_train)\n",
        "\n",
        "\n",
        "    opt_k = min(validation_error.items(), key=lambda x: x[1])[0]\n",
        "    optimal_k.append(opt_k)\n",
        "\n",
        "    print(\"The optimal value of k:\", opt_k)\n",
        "\n",
        "    predictions = kNNClassify(X_train, Y_train, test_set[0], k)\n",
        "    errors.append(1-KNNAccuracy(test_set[1], predictions))\n",
        "\n",
        "print(\"The range of the optimal values of k:\", min(optimal_k), \"to\",max(optimal_k))\n",
        "print(\"The errors for these optimal values are:\", errors)\n",
        "\n",
        "'''\n",
        "    We can choose the optimal value of k using the cross validation technique as done in part (c). If the values of rho\n",
        "    and numrep are chosen appropriately, we can get an optimal value for k.\n",
        "\n",
        "    Since the values of the error are in almost in the same range as compared to the original model, we can conclude that the\n",
        "    this solution is stable.\n",
        "'''\n",
        "\n",
        "\n",
        "#Experiments with more difficult data set. Each element flips with the following probability\n",
        "flip_probability = 0.3\n",
        "k=5\n",
        "\n",
        "random_data = data_generator(200, flip_probability)\n",
        "\n",
        "X_train = random_data[\"X_train\"]\n",
        "Y_train = random_data[\"Y_train\"]\n",
        "X_test = random_data[\"X_test\"]\n",
        "Y_test = random_data[\"Y_test\"]\n",
        "\n",
        "\n",
        "predictions = kNNClassify(random_data[\"X_train\"], random_data[\"Y_train\"], random_data[\"X_test\"], k)\n",
        "\n",
        "print(\"Accuracy of the model having noise: \", KNNAccuracy(Y_test, predictions))\n",
        "\n",
        "predict_colours = []\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "    if(Y_test[i] != predictions[i]):\n",
        "        predict_colours.append(\"red\")\n",
        "    elif(Y_test[i]==1):\n",
        "        predict_colours.append(\"green\")\n",
        "    else:\n",
        "        predict_colours.append(\"blue\")\n",
        "\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c = predict_colours)\n",
        "plt.title(\"Predicted Test Data Points\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Generate and visualize the decision regions and overlay the test points\n",
        "res = 0.1\n",
        "\n",
        "min_i, max_i = X_test[:, 0].min()-1, X_test[:, 0].max()+1\n",
        "min_j, max_j = X_test[:, 0].min()-1, X_test[:, 0].max()+1\n",
        "\n",
        "i_grid = np.arange(min_i, max_i, res)\n",
        "j_grid = np.arange(min_j, max_j, res)\n",
        "\n",
        "ii, jj = np.meshgrid(i_grid, j_grid)\n",
        "ri, rj = ii.flatten(), jj.flatten()\n",
        "ri, rj = ri.reshape((len(ri), 1)), rj.reshape((len(rj), 1))\n",
        "\n",
        "grid = np.hstack((ri,rj))\n",
        "\n",
        "grid_predictions = kNNClassify(X_train, Y_train, grid, k)\n",
        "\n",
        "kk = np.array(grid_predictions)\n",
        "kk = kk.reshape(ii.shape)\n",
        "plt.contourf(ii, jj, kk, cmap = \"bone_r\")\n",
        "\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c = predict_colours)\n",
        "plt.title(\"Decision Regions with Test Points\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUub5YBDEfX6"
      },
      "outputs": [],
      "source": [
        "##################################\n",
        "#Part(e): Influence of distance measure on KNN classifier\n",
        "##################################\n",
        "#Performance evaluation of KNN classifier with different distance measures\n",
        "\n",
        "def L2_norm(X_train, x_test):\n",
        "    return np.sqrt(np.sum(np.square(X_train-x_test.T), axis = 1))\n",
        "\n",
        "def L1_norm(X_train, x_test):\n",
        "    return np.sum(np.absolute(X_train-x_test.T), axis = 1)\n",
        "\n",
        "def norm_calculator(X_train, x_test, q):\n",
        "    return np.power(np.sum(np.power(np.absolute(X_train-x_test.T), q), axis = 1), 1/q)\n",
        "\n",
        "def kNNClassify_metric(X_train,Y_train,X_test,k,q = 2):\n",
        "    '''\n",
        "    #Inputs : Training data (X_train,Y_train), Test points  (X_test), Hyperparameter k\n",
        "    #Outputs : Predicted class\n",
        "    '''\n",
        "    predictions = []\n",
        "    possible_categories = [-1, 1]\n",
        "    for x_test in X_test:\n",
        "        x_test = x_test.reshape(x_test.shape[0], 1)\n",
        "\n",
        "        distances = norm_calculator(X_train, x_test, q)\n",
        "\n",
        "        class_distance_pair = tuple(zip(list(Y_train), list(distances)))\n",
        "        k_nearest_categories = sorted(class_distance_pair, key=lambda x: x[1])[:k]\n",
        "        categories_hash = dict(zip(possible_categories, [0]*len(possible_categories)))\n",
        "        for category in k_nearest_categories:\n",
        "            categories_hash[category[0]] += 1\n",
        "\n",
        "        max_freq = 0\n",
        "        for key in categories_hash:\n",
        "            if(categories_hash[key] > max_freq):\n",
        "                max_freq_category = key\n",
        "                max_freq = categories_hash[key]\n",
        "        predictions.append(max_freq_category)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "#Understand the decision regions of KNN classifier with different distance measures\n",
        "k=9\n",
        "\n",
        "random_data = data_generator(120)\n",
        "\n",
        "X_train = random_data[\"X_train\"]\n",
        "Y_train = random_data[\"Y_train\"]\n",
        "X_test = random_data[\"X_test\"]\n",
        "Y_test = random_data[\"Y_test\"]\n",
        "\n",
        "norms = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "accuracy = {}\n",
        "\n",
        "for q in norms:\n",
        "    predictions = kNNClassify_metric(random_data[\"X_train\"], random_data[\"Y_train\"], random_data[\"X_test\"], k, q)\n",
        "\n",
        "    accuracy[q] = KNNAccuracy(Y_test, predictions)\n",
        "\n",
        "    print(\"Accuracy of the norm\", q, \"model: \", accuracy[q])\n",
        "\n",
        "    predict_colours = []\n",
        "\n",
        "    for i in range(len(Y_test)):\n",
        "        if(Y_test[i] != predictions[i]):\n",
        "            predict_colours.append(\"red\")\n",
        "        elif(Y_test[i]==1):\n",
        "            predict_colours.append(\"green\")\n",
        "        else:\n",
        "            predict_colours.append(\"blue\")\n",
        "\n",
        "\n",
        "    #Generate and visualize the decision regions and overlay the test points\n",
        "    res = 0.1\n",
        "\n",
        "    min_i, max_i = X_test[:, 0].min()-1, X_test[:, 0].max()+1\n",
        "    min_j, max_j = X_test[:, 0].min()-1, X_test[:, 0].max()+1\n",
        "\n",
        "    i_grid = np.arange(min_i, max_i, res)\n",
        "    j_grid = np.arange(min_j, max_j, res)\n",
        "\n",
        "    ii, jj = np.meshgrid(i_grid, j_grid)\n",
        "    ri, rj = ii.flatten(), jj.flatten()\n",
        "    ri, rj = ri.reshape((len(ri), 1)), rj.reshape((len(rj), 1))\n",
        "\n",
        "    grid = np.hstack((ri,rj))\n",
        "\n",
        "    grid_predictions = kNNClassify_metric(X_train, Y_train, grid, k, q)\n",
        "\n",
        "    kk = np.array(grid_predictions)\n",
        "    kk = kk.reshape(ii.shape)\n",
        "    plt.contourf(ii, jj, kk, cmap = \"bone_r\")\n",
        "\n",
        "    print(\"The following is the decision region for the norm\", q, \"model\")\n",
        "\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1], c = predict_colours)\n",
        "    plt.title(\"Decision Regions with Test Points\")\n",
        "    plt.show()\n",
        "\n",
        "plt.scatter(accuracy.keys(), accuracy.values())\n",
        "plt.title(\"Norm vs Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "    Observations:\n",
        "    On running the code multiple times, we can observe the values of the errors are almost the same for the different norms\n",
        "    (for larger values of the data size). Hence choosing between the different norms doesn't affect the model as much\n",
        "\n",
        "    On running the code multiple times, there is not a clear and dependable pattern that is followed based on the increasing/\n",
        "    decreasing of the errors betweeen the norms.\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ogEOe3jn3aJ"
      },
      "outputs": [],
      "source": [
        "##################################\n",
        "#Part(f): Influence of distance measure on KNN classifier\n",
        "##################################\n",
        "\n",
        "#Load MNIST data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "images = mnist.data.to_numpy()\n",
        "targets = mnist.target.to_numpy()\n",
        "#Plot a few images\n",
        "plt.subplot(211)\n",
        "plt.imshow((images[0].reshape(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "plt.subplot(212)\n",
        "plt.imshow(images[1].reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1atXB22I7va"
      },
      "outputs": [],
      "source": [
        "#Create train, validation and test splits (validationn data is created in the CV function)\n",
        "\n",
        "X_train = images[:int(2*images.shape[0]/35)]\n",
        "Y_train = targets[:int(2*images.shape[0]/35)]\n",
        "X_test = images[int(69*images.shape[0]/70):]\n",
        "Y_test = targets[int(69*images.shape[0]/70):]\n",
        "\n",
        "#Write 'MultiClassKNNClassify' function\n",
        "\n",
        "def kNNClassify_mnist(X_train,Y_train,X_test,k,q = 2):\n",
        "    '''\n",
        "    #Inputs : Training data (X_train,Y_train), Test points  (X_test), Hyperparameter k\n",
        "    #Outputs : Predicted class\n",
        "    '''\n",
        "    predictions = []\n",
        "    possible_categories = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "\n",
        "    it=1\n",
        "    for x_test in X_test:\n",
        "        x_test = x_test.reshape(x_test.shape[0], 1)\n",
        "        distances = L2_norm(X_train, x_test)\n",
        "        class_distance_pair = tuple(zip(list(Y_train), list(distances)))\n",
        "        k_nearest_categories = sorted(class_distance_pair, key=lambda x: x[1])[:k]\n",
        "        categories_hash = dict(zip(possible_categories, [0]*len(possible_categories)))\n",
        "\n",
        "        for category in k_nearest_categories:\n",
        "            categories_hash[category[0]] += 1\n",
        "\n",
        "        max_freq = 0\n",
        "        for key in categories_hash:\n",
        "            if(categories_hash[key] > max_freq):\n",
        "                max_freq_category = key\n",
        "                max_freq = categories_hash[key]\n",
        "        predictions.append(max_freq_category)\n",
        "        it += 1\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "#Empirically chose most suitable k and error function based on the evauation on cross-validation data\n",
        "#Evaluate the performance using different values of 'k' on the validation data and select the optimal 'k' for the test data.\n",
        "#Evaluation the performance using different distance measures (l1,l2, etc) on the validation data and select the optimal distance measure for test data.\n",
        "def holdoutCVkNN_mnist(k_range,numrep,rho):\n",
        "    training_errors = {}\n",
        "    validation_errors = {}\n",
        "    for k in range(k_range):\n",
        "        if k % 2 == 1:\n",
        "            training_error_k = []\n",
        "            validation_error_k = []\n",
        "            for i in range(numrep):\n",
        "                random_sample = random.sample(range(len(X_train)), int(rho*len(X_train)))\n",
        "                random_sample_set = set()\n",
        "                for i in random_sample:\n",
        "                    random_sample_set.add(i)\n",
        "\n",
        "                X_training_set = []\n",
        "                Y_training_set = []\n",
        "                X_validation_set = []\n",
        "                Y_validation_set = []\n",
        "\n",
        "                for j in range(len(X_train)):\n",
        "                    if j in random_sample_set:\n",
        "                        X_validation_set.append(X_train[j])\n",
        "                        Y_validation_set.append(Y_train[j])\n",
        "                    else:\n",
        "                        X_training_set.append(X_train[j])\n",
        "                        Y_training_set.append(Y_train[j])\n",
        "\n",
        "                X_training_set = np.array(X_training_set)\n",
        "                Y_training_set = np.array(Y_training_set)\n",
        "                X_validation_set = np.array(X_validation_set)\n",
        "                Y_validation_set = np.array(Y_validation_set)\n",
        "\n",
        "\n",
        "                predictions_valid = kNNClassify_mnist(X_training_set, Y_training_set, X_validation_set, k)\n",
        "\n",
        "                validation_accuracy = KNNAccuracy(Y_validation_set, predictions_valid)\n",
        "\n",
        "                validation_error = 1 - validation_accuracy\n",
        "\n",
        "                validation_error_k.append(validation_error)\n",
        "\n",
        "            validation_errors[k] = sum(validation_error_k)/len(validation_error_k)\n",
        "\n",
        "            print(\"The error for k=\", k, \"is:\", validation_errors[k])\n",
        "\n",
        "    return validation_errors\n",
        "\n",
        "\n",
        "\n",
        "#Plot training and validation errors for different values of k\n",
        "\n",
        "k_range = 12\n",
        "numrep = 1\n",
        "rho = 1/4\n",
        "\n",
        "validation_errors = holdoutCVkNN_mnist(k_range, numrep, rho)\n",
        "\n",
        "plt.scatter(validation_errors.keys(), validation_errors.values(), c = \"blue\", label = \"Validation Error\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Errors in ascending order: \", dict(sorted(validation_errors.items(), key=lambda item: item[1])))\n",
        "\n",
        "\n",
        "\n",
        "#Evaluate the performance on test data with the best hyper parameters ( k, error_func ) obtained from cross validation\n",
        "#The error function is empirically chosen to be #misclassified/#data_points, since the weight of all the misclassified data is to be the same\n",
        "\n",
        "\n",
        "k = min(validation_errors, key=validation_errors.get)\n",
        "predictions = kNNClassify_mnist(X_train, Y_train, X_test, k)\n",
        "print(\"The optimal value of k is\", k)\n",
        "print(\"The accuracy for this value of k:\", KNNAccuracy(Y_test, predictions))\n",
        "\n",
        "\n",
        "\n",
        "#Create a confusion matrix for test data\n",
        "def compute_confusion_matrix(true, pred):\n",
        "    '''\n",
        "    Inputs: Ground truth labels and classifier predictions\n",
        "    Outputs: Confusion matrix\n",
        "    '''\n",
        "    confusion_matrix = np.zeros((10, 10))\n",
        "\n",
        "    for i in range(len(true)):\n",
        "        confusion_matrix[int(true[i])][int(pred[i])] += 1\n",
        "\n",
        "    return confusion_matrix\n",
        "\n",
        "print(\"The confusion matrix for the optimal value of k:\")\n",
        "print(compute_confusion_matrix(Y_test, predictions))\n",
        "\n",
        "'''\n",
        "    Ways to improve performance:\n",
        "        Using a weighted distance metric, based on the pixel positions on the image. This can give more weight to certain\n",
        "        regions of the image and hence can help classify better\n",
        "\n",
        "        Using a convolution transform, to give more weights to spatial similarity\n",
        "\n",
        "        Using machine learning models that are optimized for image classification, such as convolutional neural networks\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OohdgUOoAenj"
      },
      "source": [
        "<b> Report your observations </b>\n",
        "1. Write down the best accuracy on synthetic test data generated from Gaussian distribution:<br>\n",
        "72.5%\n",
        "\n",
        "2. Write down the best accuracy on MNIST validation and test data:<br>\n",
        "Validation: 94.5%, Test: 95.3%\n",
        "\n",
        "3. Report your observations on the confusion matrix of KNN classifier on MNIST test data:<br>\n",
        "Highly Misclassified integer: 2<br>\n",
        "High accuracy classified integer: 1<br>\n",
        "   Most integers mistaken for: 9\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}